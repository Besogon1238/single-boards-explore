# Дневник погружения в работу с одноплатниками
Данная страничка создана в целях отражения моего собственного изучения работы с одноплатниками и ALTLinux на этих одноплатниках. В перспективе последующим применением оставленных здесь записей может стать формирование методических материалов для студентов, приходящих в лабораторию.



Я буду очень благодарен любым комментариям, замечаниям и дополнениям к моим заметкам, которые будут здесь публиковаться. Принимаю любые косяки и недочеты, как по содержанию, так и по форме :)

Мои контакты для связи — почта (taranev@basealt.ru) и телеграмм (@tar_xjvf).


## Оглавление
- [Введение](#intro)
- [Старт работы с одноплатниками](#start)
  - [Установка образа операционной системы](#install)
  - [Включение и начало работы с платой](#boot)
- [Написание кода под RISCV-архитектуру](#coding)
  - [Кросс-компилятор](#cross-compiler)
  - [IDE или не IDE](#ide)
  - [Настройка сборки через IDE](#compile_in_IDE)
  - [Запуск бинарника на плате](#test_running)
  - [Сборка в изолированной среде (hasher)](#compile_in_hasher)
  - [Запуск бинарника в hasher](#running_in_hasher)
  - [Отладка в hasher](#debugging_in_hasher)
  - [Рабочие способы отладки](#working_debugging)
- [Идеи применения одноплатников](#ideas)
  - [Использование в качестве MQTT-клиента](#mqtt-client)
- [Подробнее про образы ОС](#image_deep)
  - [Личный опыт](#self_expirience_in_image)
- [Потенциально полезные заметки](#else_useful_info)


<a name="intro"></a>
## Введение

Стоит сказать, что я до последнего момента мало сталкивался с разработкой, заточенной под архитектуру, отличающуюся от x86_64. Поэтому многое может быть для случайного читателя максимально очевидным, что для меня очевидным не являлось.

Стоит ввести некоторые вводные. 

Имеется: 

* лаборатория с отдельной локальной сетью по ethernet и wi-fi;
* десять стационарных компьютеров, на которых установлены ALT Рабочие Станции 11;
* 13 одноплатников Sipeed Lichee RV
* 12 одноплатников Mango PI
* Много мишуры для того, чтобы работать с одноплатниками: видеокабели, удлинители USB, UART переходники, карты памяти
* желание освоить написание кода под одноплатники и поделиться собственным опытом<br />
Конечным артефактом моего импровизированного дневника в моем представлении должен стать некоторый перечень лабораторных работ, который может помочь случайному читателю комфортно погрузиться в темы, которые заинтересуют меня.

<a name="start"></a>
## Старт работы с одноплатниками

В распоряжении лаборатории есть 2 типа одноплатников на основе микропроцессора Allwinner D1 [Sipeed](https://www.sipeed.com/) [Lichee RV](https://wiki.sipeed.com/hardware/en/lichee/RV/RV.html) и [MangoPi](https://mangopi.org) [MQ Pro](https://mangopi.org/mangopi_mqpro).

Первый вопрос, который возникает "Как их запустить?".  Довольно очевидно, что нужен носитель с операционной системой(в нашем случае карта памяти) и образ операционной системы.

<a name="install"></a>

### Установка образа операционной системы на носитель 
![Картридер](/pictures/картридер.jpg)

С образом операционной системы все довольно просто. 

Как я понял, обычно производитель платы предоставляет тестовый образ для демонстрации ее возможностей. В моем же случае из-за моего желания запустить на платах ALT возникла проблема отсутствия такого образа. С этим мне помог Иван Мельников, который собрал [такой образ](https://ftp.altlinux.org/pub/people/iv/images/riscv64/regular-sunxi-riscv64/) для Lichee RV.

Но как записать образ на карточку? На самом деле все довольно просто:

Распакуем архив с образом

```
$ xz -dk /путь/к/архиву
```

Затем, аккуратно вставляем флешку в **картридер**.

Узнаем с помощью lsblk название устройства, и если на нем уже есть точки монтирования, размонтируем.

```
$ lsblk
# umount /dev/sdX* # если что то было примонтировано
```

С помощью утилиты dd запишем образ на флешку:

```
# dd if=путь/к/образу.img of=/dev/sdX bs=4M status=progress
```

После указанных манипуляций мы должны получить карту памяти с готовой операционной системой.

<a name="boot"></a>

### Включение и начало работы с платой 

Поскольку нам необходимо работать с платой, а платы у нас достаточно малоресурсные, чтобы работать на графике(хотя с данным образом Lichee RV ее поддерживает) нам нужно работать через консоль. Для работы с консолью пригодиться **UART-преобразователь**, подключаемый на три контакта  GPIO платы. Контакты TX и RX у Lichee RV подписаны на задней стороне платы, а "гребенка" Mango соответствует спецификации "гребенок" Raspberry PI.

![Джампер](/pictures/джампер%20на%20перехооднике.jpg)

Важно правильно выбрать на UART-преобразователе джампером напряжение. В нашем случае, для обоих одноплатников это 3.3V. 

Если выбрать напряжение ниже необходимого — передача данных просто не будет возможной, а вот если выбрать выше, то появится хорошая возможность спалить устройство. **ГОВОРЮ НА СОБСТВЕННОМ ОПЫТЕ, ВНИМАТЕЛЬНЕЕ!**

Подключаем UART-преобразователь к компьютеру. Для удобства может пригодиться usb-удлинитель, если ПК стационарный. Далее, соединяем пин GND на GPIO контактах с пином GND на преобразователе, а RX пины соединяем с TX пинами, и наоборот.

Теперь понадобится программа для взаимодействия с платой. Например можно воспользоваться tio:

```
# apt-get install tio
```

![UART-преобразователь](/pictures/соелинение%20переходника%20с%20GPIO%20ПЛАТЫ.jpg)

Чтобы определить имя устройства в системе можно почитать логи dmesg. Наприме так:

```
dmesg | grep -i "tty"
```

Подключимся к UART-преобразователю, и ждем поступления сигналов

```
$ tio -b 115200 /dev/ttyUSBX
```

Теперь если вставить флеш-карточку с записанной на нее образом в соответствующий интерфейс на плате, можно увидеть в консоли логи запуска операционной системы, а затем и приглашение ко вводу.

На всех регулярных сборках ALT регулярных сборках, до прохождения мастера первоначальной настройки, логинка *root* с паролем *altlinux*.

<a name="coding"></a>

## Написание кода под RISCV-архитектуру
Я оказался в точке, в которой

* есть плата с архитектурой RISCV, под которую нужно писать программный код;
* есть ПК на архитектуре x86_64, на котором удобно писать программный код;
* есть сделать максимально комфортным процесс разработки и отладки кода;

Такие обстоятельства вынудили меня выяснить, что я могу сделать, чтобы облегчить себе жизнь.

Дальше возник вопрос, а собственно, как организуют работу с кодом люди, которые постоянно разрабатывают программный код, поставляемый на разные процессорные архитектуры. Я снова обратился к интернету и опыту коллег. Выяснилось, что делать это можно по-разному. Принцип, разумеется везде один:

* пишется код
* компилируется кросс-компилятором
* так или иначе переносится на целевую платформу ИЛИ целевая платформа эмулируется
* код запускается и отлаживается

<a name="cross-compiler"></a>

### Кросс-компилятор

Первое, что оказалось необходимым — **кросс-компилятор**. Кросс-компилятор — это специальный компилятор, который позволяет компилировать код для платформы, отличной от той, на которой он выполняется.

Нужен кросс-компилятор? [Их есть у Альта!](https://packages.altlinux.org/en/sisyphus/srpms/cross-toolchain-riscv64-linux-gnu/) 

Ставим:

```
# apt-get install gcc-riscv64-linux-gnu
```

Прекрасно. Кросс-компилятор — есть.

<a name="ide"></a>

### IDE или не IDE

Нужно решить, где писать программный код. Мой собственный, не самый долгий опыт разработки всегда был связан с разработкой кода с помощью IDE, не все мои коллеги разделяют такой подход. Кому то привычно быть независимым от возможностей графического отображения всего подряд и обходиться без среды разработки. Для такого подхода имеются [основания](https://habr.com/ru/articles/303554/), и, быть может, я к этому тоже приду через время. Но в данный момент мне самому, и, думаю, многим читающим мои записи привычно использовать IDE, поэтому я решил не отказывать своей привычке. 

В прочем, все оказалось не так уж и просто. О чем можно будет узнать далее.

Я решил вести разработку в привычной мне среде Eclipse. С установкой никаких проблем: [установочник доступен на официальном сайте](https://www.eclipse.org/downloads/packages/installer). В профиле установочника выбираем *Eсlipse IDE for Embedded C/C++ Developers*.

<a name="compile_in_IDE"></a>

### Настройка сборки через IDE

Поскольку я выбрал быть счас...разработчиком, который пишет код в IDE это влечет за собой необходимость эту IDE настроить. Вернее, настраивать будем не IDE, а параметры, которые будут переданы *Make*.

Make — это утилита для автоматизации сборки программного обеспечения. Она читает инструкции из файла Makefile и выполняет компиляцию, линковку и другие задачи в зависимости от изменений в исходном коде.

Первое с чем предстоит столкнуться - это заставить *make* использовать для наших манипуляции с кодом **тулчейн**, который мы установили вместе с пакетом *gcc-riscv64-linux-gnu*.

**Тулчейн (Toolchain)** — это набор инструментов (утилит, компиляторов, библиотек), которые используются для разработки и сборки программного обеспечения под определенную платформу или архитектуру.

Cоздаем пустой проект C/C++ > C Managed Build > Hello World RISC-V C++ Project > RISC-V Cross GCC > Toolchain name = что_бы_то_ни_было & toolchain path = /usr/bin.

Можно попробовать собрать проект, и разумеется, ничего не выйдет поскольку *make* понятия не имеет, что такое *что_бы_то_ни_было*, которое мы выбрали в качестве имени тулчейна. Идем в Properties проекта > C/C++ Build > Settings > Toolchains. Вот здесь мы и настроим наш уже установленный в папку */usr/bin* тулчейн. Имя тулчейна можно оставить любое, архитектура RISCV, префикс устанавливаем *riscv64-linux-gnu-*, компиляторы gcc и g++. Затем проверяем, что toolchain path *usr/bin*, и применяем настройки. Теперь Make будет знать, где лежат и как называются все компоненты установленного тулчейна. Пробуем построить проект. Все должно получиться.

<a name="test_running"></a>

### Тестирование запуска бинарника на плате

Итак, теперь, наше *Hello World* приложение собрано для интересующей нас архитектуры. Можно попробовать ее исполнить, к чему и приступим.

Для этого авторизуемся в root на плате и настроим сеть

```
# nmcli dev show # узнаем как называется wifi-интерфейс
# nmcli dev wifi # узнаем доступные wifi сети
# nmcli dev wifi connect "название_сети" password "пароль_сети"

# ping ya.ru # проверим, что у появился DNS и выход в интернет

# ip a # узнаем текущий айпишник платы

```

<a name="ssh"></a>

Если плата находиться в одной сети с ПК, значит, можно попробовать подклюиться по ssh. Если до этого момента у вас есть только учетная запись root, необходимов в /etc/openssh/sshd_config расскомментировать параметр PermitRootLogin и поставить после его обяъявления значение "yes".

```
# apt-get install nano  # поставим редактор(если его не было)
# nano /etc/openssh/sshd_config # исправляем параметр(если надо)
# systemctl restart sshd # перезапустим, sshd
# systemctl status sshd # проверим, работает ли ssh
```

Если все хорошо, то можно передать скомпилированный бинарник на целевое устройство и там его выполнить

```
# scp /путь/до/бинарника имя_пользователя@айпи.нашей.riscv.платы:/куда/положить # пользователь по умолчанию root, пароль altlinux
```

Остается выполнить бинарник и порадоваться.

<a name="compile_in_hasher"></a>

### Сборка в изолированный среде

Прекрасно! Но у меня сразу созрел вопрос. А как мне вести разработку дальше? Ведь, если писать не *Hello World* приложение, а что то чуть более серьезное, то появиться необходимость постоянно собирать программу вместе с различными заголовочными файлами, которые не входят в стандартную библиотеку. Кроме того, чтобы проверить работу приложения каждый раз нужно будет руками или каким то скриптом пересылать бинарник на плату и там его выполнять. Мне захотелось найти путь, по которому от этих неудобств можно избавиться.

Я спросил совета у опытного человека, который дал мне ценную идею и способ ее реализации. И, скажу откровенно, меня вся эта схема впечатлила. Не догадывался, что такое возможно. Данный способ обусловлен использованием связки двух технологий: [hasher](https://www.altlinux.org/Hasher) и [QEMU](https://ru.wikipedia.org/wiki/QEMU).

Стоит внести краткий вводные. 
*QEMU* – это эмулятор, который может выполнять код, предназначенный для одной архитектуры процессора, на другой. *qemu-user-static* – версия *QEMU*, работающая без полной эмуляции всей системы, обеспечивающая только исполнение бинарников.

*hasher* — это инструмент безопасной и воспроизводимой сборки пакетов. Инструмент спроектирован так, чтобы не допускать влияния собираемого пакета на хост-систему, а также взаимного влияния собирающихся пакетов.

В этой схеме qemu-user-static будет применяться по прямому назначению для запуска собранных бинарников, а hasher будет обеспечивать  изолированную среду, в которой будут тестироваться собранные пакеты, симулируя тем самым файловую систему реальной железки. Такая схема позволить запускать собираемые бинарники без их пересылки на реальное устройство и не беспокоится о коллизиях, которые могут возникнуть в рантайме приложения.

Отдельно отмечу, поскольку один из читателей дневника сакцентировал внимание на этом моменте, что *hasher* во всей этой машинерии нужен, чтобы обеспечить среду приближенную к реализации и исполнению на реальной железяке с соответствующей *чистой* файловой системой, пакетами и архитектурой.

Если описать кратко перечень необходимых действий, то требуется следующая последовательность:

* Установить пакет qemu-user-static-binfmt-riscv
* Установить hasher и настроить его
* Поставить в hasher то, что нужно для сборки программы
* Передать chroot hasher мейку в качестве sysroot для кросс-компилятора
* Проверить результат работы в хэшере

Начнем с самого простого :)

```
# apt-get install qemu-user-static-binfmt-riscv
```

Теперь установим и настроим hasher.Предлагаю обратиться к [руководству по hasher](https://www.altlinux.org/Hasher/Руководство). По данному руководству необходимо выполнить некоторые конкретные пункты:

* Установка
* Добавление пользователя

После этого

```
$ mkdir ~/hasher # создаем директорию для сборочной среды(можно выбрать любое место, но на tmpfs будет быстрее см.руководство)
```

Далее создаю в папке ~/apt(или любой другой) файлы apt.conf.riscv64-pve и sources.list.riscv-pve. Данная конфигурация укажет хэшеру, откуда брать пакеты для установку в среду. Приведу содержимое файлов.

apt.conf.riscv64-pve
```
Dir::Etc::main "/dev/null";
Dir::Etc::parts "/var/empty";
Dir::Etc::SourceParts "/var/empty";
Dir::Etc::sourcelist "/ваш/хомяк/apt/sources.list.riscv-pve";

RPM::Ignore { "vim-plugin-vimruby"; };
```

sources.list.riscv-pve

```
rpm [sisyphus-riscv64] http://ftp.altlinux.org/pub/distributions/ALTLinux/ports/riscv64 >

rpm [sisyphus-riscv64] http://ftp.altlinux.org/pub/distributions/ALTLinux/ports/riscv64 >
```

После чего создаем окружение явно с указанием архитектуры, пути к конфигу для пакетного менеджера

```
$ hsh --init --target riscv64 --apt-conf ~/hasher/apt/riscv64-pve.conf ~/папка/с/вашим/окружением/hsh-rv64
```

Далее, если нам необходим или будет необходим какой то пакет внутри хэшера можно воспользоваться следующей командой, чтобы поставить в hasher нужное:

```
$ hsh-install ~/папка/с/вашим/окружением/hsh-rv64 название-необходимого-пакета
```

Кроме того, теперь у нас появилась возможность войти в окружение хэшер, и например выполнить в нем какой то скрипт или бинарник.

```
$ hsh-shell ~/папка/с/вашим/окружением/hsh-rv64
```

Фуууух. Дело осталось за малым. Нужно научить мейк передавать chroot в качестве sysroot кросс-компилятору и указывать компилятору откуда брать библиотеки. 

Решаем вопрос с библиотеками:

Перейдем в Настройки проекта > C/C++ Build > Settings > Tool Settings > GNU RISC-V CROSS C Compiler > Includes

```
Include paths = /путь/до/чрута/хэшера/usr/include
```

Перейдем в Настройки проекта > C/C++ Build > Settings > Tool Settings > GNU RISC-V CROSS C Linker > Libraries

```
Library search path = /путь/до/чрута/хэшера/usr/lib
```

Укажем chroot хэшера в качестве sysroot: 

Перейдем в Настройки проекта > C/C++ Build > Settings > Tool Settings > GNU RISC-V CROSS C Compiler > Command

```
Command=${cross_prefix}${cross_c} --sysroot ~/hasher/hsh-rv64/chroot ${cross_suffix}
```

<a name="running_in_hasher"></a>

### Исполнение программы в изолированной среде 

Для того, чтобы что-то выполнить в hasher, это что-то нужно туда поместить. Я решил поступить в лоб, и после компиляции бинарника копировать его в корень chroot хэшера, указав это в параметре make, в настройка сборки.

```
Post-build steps Command=cp ~/eclipse-workspace/путь/к/созданному/бинарнику.elf ~/путь/к/chroot
```
Пора настроить запуск нашего бинарника: в Run Configuration проекта указываем в параметре C/C++ Application 

```
/абсолютный/путь/в/chroot/бинарник.elf
```
 
На кураже совсем забыл указать, а зачем во всей это схеме нужен был пакет qemu-user-static-binfmt-riscv. Спасибо sorochaniv@basealt.ru за внимание к этому подразделу. Данный пакет позволяет запускать бинарники, скомпилированные под другую архитектуру. 

Только чтобы все заработало обязательно нужно во вкладке Environment указать значение переменной окружения. Спасибо за подсказку от *опытного коллеги*.

```
QEMU_LD_PREFIX=/абсолютный/путь/к/chroot
```

Данная переменная укажет,где находится динамический загрузчик (в нашем случае '/lib64/ld-linux-riscv64-lp64d.so.1'), который загружает и связывает динамические библиотеки при запуске программы.

Если все манипуляции были выполнены программу можно будет выполнить в hasher с помощью hsh-run, или зайдя в его окружение с помощью hsh-shell, или прямо из IDE, как на картинке.
![Запуск в IDE](/pictures/Запуск%20в%20IDE.png)



Теперь я почти счастлив. Была бы еще отладка...но с этим все несколько сложнее и надо разбираться :)

<a name="debugging_in_hasher"></a>

### Отладка в изолированный среде

А она де-факто для нашего случая просто невозможна. Существует возможность [отлаживать что либо в hasher под x86_64](https://www.altlinux.org/Hasher/gdb) архитектуру. Но использовать такой способ для целевой архитектуры отличной от хостовой не получится. 

Если предпринять попытку запустить gdb в hasher то непременно попадешь в проблему отсуствия возможности использовать [ptrace вызовы](https://habr.com/ru/companies/otus/articles/898448/).

Получается, что в изолированной среде отлаживать, что либо получится только printf-отладкой, что меня конечно не устраивает. Этот метод зачастую неудобен и не отражает того, что происходит в полной мере.

<a name="working_debugging"></a>

### Рабочие методы отладки

Исходя из выводов предыдущего раздела, по существу, есть два варианта для отладки программы под наши riscv64 платы, если она понадобится:

* Отлаживать программу, подключаясь к плате и работая с консольным интерфейсом gdb
* Отладка с использованием ssh плагина для VSCodium

Рассмотрю подробнее про второй способ, о котором я узнал от опытного товарища, и который показался мне более удобным.

#### Подключение к плате с помощью плагина Remote-SSH

Начнем с первого варианта, о котором мне рассказал мой более опытный товарищ. Существует такой плагин для редактора исходного VS Code, который называется "Open Remote - SSH". Он, используя протокол ssh, позволяет писать, компилировать и отлаживать код на удаленной машине, работая на основной(разумеется, с издержками). Я **обращаю внимание**, что поддержка riscv64 есть только в неофициальном плагине для **VSCodium**. И с ним есть небольшая загвоздка, о который расскажу далее.

Все, что требуется для работы с этим плагином уже описано [выше](#ssh). Если вы можете подключиться к плате по ssh и выполнять команды из консоли. Значит, на бумаге, вы готовы.

```
# apt-get install codium # ставим редактор кода
```

Устанавливаем расширение для среды разработки, и жмем комбинацию *Ctrl + Shift + P*. В появившемся окне выбираем *Remote-SSH: Connect to Host*. Далее, все идет по стандартному сценарию подключения по ssh с использованием пароля: вводим цель подключения в формате имя_пользователя@ip_адрес, после чего вводим пароль. Вуаля! После небольшого ожидания будет получен доступ к файлам выбранного пользователя(в моем случае это root). Остается только настроить среду.

Все бы хорошо, но есть парочка НО.

* Перед попыткой подключения стоит убедиться, что на целевой плате установлен gcc и его библиотеки. 
* Проблема с работой для [образа](https://ftp.altlinux.org/pub/people/iv/images/riscv64/regular-sunxi-riscv64/), который был предложен к использованию

Суть проблемы следующая. При успешном подключение по ssh по пути /$HOMEDIR/.vscodium-server/bin/ устанавливается баш скрипт, который выкачивает [отсюда](https://github.com/VSCodium/vscodium/releases/download/1.99.32562/vscodium-reh-linux-riscv64-1.99.32562.tar.gz)  *vscode-server* и разрешает его запускать. Но зачастую на каком то моменте все обламывается и подключения не происходит, хотя архив загружен и распакован в нужное место.

Через некоторое время я выяснил, что, по видимому, вся проблема заключается в том, что при подключении плагин загружает переменные окружения из shell удаленной машины и просто не дожидается ответа. За время на ответ отвечает параметр shellEnvironmentResolutionTimeout* из настроек VSCodium. Его увеличение позволяет решить проблему.

P.S. Иногда все равно требуется 2 попытки подключения...

После этого отлаживать код на Lichee RV и Mango PI станет возможным, но с кокретными ограничениями в производительности. Скомпилировать и запустить Hello-World программу можно приблизительно за 10~ секунд :)

P.S Чтобы начать работать на удаленной машине в открытом новом окне редактора идем в *Главное меню* > *File* > *Open Folder*.

#### Компиляция, запуск и отладка C/C++ проектов на удаленной машине

На данном этапе каждому предоставлен полный карт-бланш на настройку окружения разработки и редактора кода. Я поделюсь собственным наиболее удобным для себя сетапом. Прежде чем перейти к его описанию напомню, что на этом шаге на плате уже должен быть установлен gcc, а также необходимо установить отладчик gdb из репозитория.

После того, как вы вошли по ssh на плату и выбрали место для своего нового проекта, вам нужно установить два расширения для VSCodium сервера на плате: *С/C++ Runner* упрощающий сборку проектов без ручного конфигурирования конфига [tasks.json](https://code.visualstudio.com/docs/debugtest/tasks) и *CDT GDB Debugger* для интеграции с gdb на плате.

Протестировать, как работает компиляция, запуск и отладка на плате с предложенными мной расширениями VSCodium можно на самом простом примере. Создадим папку проекта, в ней создадим каталог *src* и внутри создадим файл .cpp:

```
#include <iostream>
using namespace std;

int main() {

    int tmp = 0;

    cout << "Hello, VS Code C++!" << endl;
    return 0;
}
```

После установки расширения *С/C++ Runner* в нижней панели редактора появится желтая надпись с предложением выбрать папку проекта. В ней достаточно указать путь к каталогу *src*. Вообще, что я тут понаписал можно не читать и более подробно почитать понятную [документацию к расширению](https://marketplace.visualstudio.com/items?itemName=franneck94.c-cpp-runner).

![alt text](/pictures/extension_panel.png)

Теперь по нажатию кнопки *Start compilation* в папке проекта должна появиться папка *.vscode*, в которой будут содержаться автоматически сгенерированные .json файлы настройки компиляции и запуска программы(их можно редактировать и руками), а в каталоге *src* появится папка *build* с скомпилированным объектным файлом и бинарником(по умолчанию будет называться outDebug или outRelease в зависимости от выбранного в нижней панели режима компиляции).

Запуск бинарника должен пройти без проблем. А вот авитоматически сгенерированные настройки для отладки, в моем случае, пришлось поднастроить.

По умолчанию, для отладки расширение использует конфигурацию ![alt text](/pictures/debug_conf_default.png)

Жмем на *кнопку выбора конфигурации для отладки* > *Add configuration* > в раскрывшемся контекстном меню *GDB CDT Local Debugging*. Получим такую новую конфигурацию в файле */.vscode/launch.json*:

```
{
  "type": "gdb",
  "request": "launch",
  "name": "Name here",
  "program": "${workspaceFolder}/${command:askProgramPath}"
}
```

Немного видоизменим его:

```
{
  "type": "gdb",
  "request": "launch",
  "name": "Имя_конфигурации_которое_вам_нравится",
  "program": "${workspaceFolder}/путь/до/сгенерированного/бинарника"
}
```

Готово! Теперь можно выбрать конфигурацию с заданным именем, поставить точки останова или просто повыполнять программу пошагово, чтобы проверить, как работает отладчик.

Хотел было я на этом и закончить со своим сетапом, но не смог. Я понял, что мне лично хотелось бы обеспечить себе подсветку синтаксиса, возможность переходить к определению и объявлению функций, переменных и тд.

Поэтому мне пришлось обеспечить себя работой еще одного популярного расширения для VSCodium [*Clangd*](https://marketplace.visualstudio.com/items?itemName=llvm-vs-code-extensions.vscode-clangd) собственно представлющую собой обертку для работы с [clangd](https://clangd.llvm.org/).

Для начала, поставим из репозитория пакета clang и clangd. Далее, устанавливаем на удаленную машину расширение, указываем параметр **clangd.path = /usr/bin/clangd**.

Параметр можно найти по пути *File > Preferences > Settings > Remote [SSH: ip.адрес.нашей.платы ]> Extensions > clangd*.

Далее необходимо вызвав *Ctrl+alt+p* вызвать функцию активации расширения, и подождать пока расширение свяжется с Clangd-сервером и проиндексирует проект.

Вот теперь можно с комфортом можно делать все и сразу на удаленной машине!

P.S. Утановив расширение VSCodium для Python также можно работать и на нем, но не дебажить :(.

<a name="ideas"></a>

## Идеи применения одноплатников

В данном подразделе планируется фиксировать всевозможные идеи(а может и реализацию) практического применения одноплатников в учебных целях.

<a name="mqtt-client"></a>

### Использование в качестве MQTT-клиента/брокера

MQTT (Message Queuing Telemetry Transport) – это легковесный протокол для обмена сообщениями между устройствами, работающий по модели «издатель-подписчик» (pub/sub). Разработан в 1999 году для мониторинга нефтепроводов, но сегодня широко используется в IoT, умных домах, промышленной автоматизации и других сферах, где важны низкое энергопотребление и работа в нестабильных сетях.

#### Основные компоненты

- **Брокер (Broker)** — центральный сервер, который принимает сообщения от **издателей (publishers)**, пересылает их **подписчикам (subscribers)** по нужным топикам.

- **Клиенты** — устройства или приложения которые могут обладать следующими ролями(причем иногда одновременно)
    - **Издатель** — отправляет данные в брокер
    - **Подписчик** — получает данные по подписке
- **Топики** — адреса, по которым передаются сообщения. Иерархическая структура через **/**: 
    - *home/kitchen/temperature*
    - *factory/machine1/status*

Более подробную информацию о протоколе и его возможностях, например про шаблоны подписки и качество обслуживания, предлагаю читателю найти самостоятельно :)

Данный протокол широко применяется в умных домах, различных системах мониторинга и телеметрии.

#### Пример реализации

Идея применения одноплатников путем использования проста — к одноплатникам можно подключить различные датчики, обрабатывать с них данные, а затем уже отправлять по подписке брокеру и другим клиентам для дальнейшей обработки и визуализации на более мощной машине.

Приведу самую простую реализацию издателя на C (для одноплатника) и брокера-подписчика на Python (для ПК). 

Вводные прежние: есть одноплатник lichee_rv с Альтом на борту, и x86_64  ПК.

Для написания клиента нам пригодиться библиотека paho-c, разработанную для реализации клиентской части протокола MQTT. Для установки на клиенте необходимо выполнить на одноплатнике:

```
# apt-get install libpaho-mqtt1 libpaho-mqtt-devel
```

Для написания брокера, выполняющего одновременно функцию подписчика будем использовать питоновскую реализацию paho.

```
# apt-get install python3-module-paho
```

В моем примере брокер-подписчик, выступая в качестве брокера в отдельном потоке высылает в свою локальную сеть бродкаст-запросы, чтобы дать возможность клиентскому устройству понять, куда ему подключаться.

Выступая же в качестве клиента он подписывается на топик *lichee_rv/stats*, и при получении данных просто выводит их в консоль.

<details>

<summary>Вспомогательные функции брокера-подписчика</summary>

<pre><code>
import json
import time
import socket


def on_connect(client, userdata, flags, rc):
    print(f"Connected with result code {rc}")
    client.subscribe("lichee_rv/stats")

def on_message(client, userdata, msg):
    try:
        payload = msg.payload.decode()
        print(f"Raw message received: {payload}")
        data = json.loads(payload)
        print(f"Parsed data: {data}") 
    except Exception as e:
        print(f"Error processing message: {e}")


def get_local_ip():
    try:
        sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        sock.connect(("8.8.8.8", 80))  # Google DNS
        ip = sock.getsockname()[0]
        sock.close()
        return ip
    except Exception as e:
        print(f"Error getting IP: {e}")
        return "127.0.0.1"


def broadcast_broker_ip():
    sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
    sock.setsockopt(socket.SOL_SOCKET, socket.SO_BROADCAST, 1)
    
    BROADCAST_PORT = 54545

    current_ip = get_local_ip()
    message = f"MQTT_BROKER:{current_ip}:1883".encode()
    
    while True:
        try:
            sock.sendto(message, ('255.255.255.255', BROADCAST_PORT))
            print(f"[UDP] Sent broadcast: {message.decode()}")
        except Exception as e:
            print(f"[UDP] Broadcast error: {e}")
        time.sleep(5) 

</code></pre>

</details>


<details>

<summary>Главная функция брокера-подписчика</summary>

<pre><code>
import threading
import paho.mqtt.client as mqtt
from broker_utils import on_connect,on_message,get_local_ip,broadcast_broker_ip

threading.Thread(target=broadcast_broker_ip, daemon=True).start()

client = mqtt.Client()
client.on_connect = on_connect
client.on_message = on_message

broker_ip = get_local_ip()

try:
    client.connect(broker_ip, 1883, 60)
    print("Connecting to broker...")
    client.loop_forever()
except KeyboardInterrupt:
    client.disconnect()
    print("Disconnected")
except Exception as e:
    print(f"Connection error: {e}")

</code></pre>


</details>

В свою очередь клиент-издатель, ищет в сети брокера, подключается к нему. После чего вычисляет текущую нагрузку на ЦП и использование оперативной памяти(ну а что еще измерять, если нету датчиков) и отправляет с нужным топиком.


<details>

<summary>Главная функция клиента-издателя</summary>

<pre><code>

#include <stdio.h>
#include "client_utils.h"

int main() {

    char* broker_ip = discover_broker();
    if (!broker_ip) {
        fprintf(stderr, "Failed to discover broker\n");
        return 1;
    }

    printf("Discovered broker at %s\n", broker_ip);

    struct mosquitto *mosq = mosquitto_new(NULL, true, NULL);
    if (mosquitto_connect(mosq, broker_ip, MQTT_PORT, KEEPALIVE)) {
        fprintf(stderr, "MQTT connection failed\n");
        free(broker_ip);
        return 1;
    }

    char payload[128];
    float cpu, ram;

    mosquitto_lib_init();

    if (!mosq) {
        fprintf(stderr, "Error: Out of memory.\n");
        return 1;
    }

    if (mosquitto_connect(mosq, broker_ip, MQTT_PORT, KEEPALIVE)) {
        fprintf(stderr, "Unable to connect to MQTT broker.\n");
        return 1;
    }

    free(broker_ip);

    printf("MQTT client started.\n");

    while (1) {
        cpu = get_cpu_usage();
        ram = get_ram_usage();

        if (cpu < 0 || ram < 0) {
            fprintf(stderr, "Error reading system stats\n");
            sleep(1);
            continue;
        }

        snprintf(payload, sizeof(payload),"{\"cpu\":%.2f,\"ram\":%.2f}", cpu, ram);

        int ret = mosquitto_publish(mosq, NULL, MQTT_TOPIC,strlen(payload), payload, 0, false);

        if (ret != MOSQ_ERR_SUCCESS) {
            fprintf(stderr, "Error publishing: %s\n", mosquitto_strerror(ret));
        } else {
            printf("Sent: %s\n", payload);
        }

        sleep(1);
    }

    mosquitto_destroy(mosq);
    mosquitto_lib_cleanup();
    return 0;
}


</code></pre>

</details>


<details>
<summary>Вспомогательные функции клиента-издателя</summary>

<pre><code>
#ifndef CLIENT_H
#define CLIENT_H
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <unistd.h>
#include <mosquitto.h>
#include <sys/socket.h>
#include <netinet/in.h>
#include <arpa/inet.h>

#define BROADCAST_PORT 54545
#define BROADCAST_MAGIC "MQTT_BROKER:"

#define MQTT_PORT 1883
#define MQTT_TOPIC "lichee_rv/stats"
#define KEEPALIVE 60
#endif

static char* discover_broker() {
    int sock = socket(AF_INET, SOCK_DGRAM, 0);
    if (sock < 0) {
        perror("UDP socket error");
        return NULL;
    }

    // Allow broadcast messages
        int broadcast_enable = 1;
    setsockopt(sock, SOL_SOCKET, SO_BROADCAST, &broadcast_enable, sizeof(broadcast_enable));

    // Configure reciever address
    struct sockaddr_in addr;
    memset(&addr, 0, sizeof(addr));
    addr.sin_family = AF_INET;
    addr.sin_port = htons(BROADCAST_PORT);
    addr.sin_addr.s_addr = INADDR_ANY;

    if (bind(sock, (struct sockaddr*)&addr, sizeof(addr)) < 0) {
        perror("UDP bind error");
        close(sock);
        return NULL;
    }

    printf("Listening for broker broadcasts...\n");
    char buffer[256];
    int len = recv(sock, buffer, sizeof(buffer) - 1, 0);
    close(sock);

    if (len <= 0) {
        perror("UDP receive error");
        return NULL;
    }

    buffer[len] = '\0';
    printf("Received broadcast: %s\n", buffer);

    // Verify that messagw from broker
    if (strstr(buffer, BROADCAST_MAGIC) != buffer) {
        fprintf(stderr, "Invalid broadcast message\n");
        return NULL;
    }

    // Parsing ip and port from format "MQTT_BROKER:IP:PORT")
    char* ip = buffer + strlen(BROADCAST_MAGIC);
    char* port_str = strchr(ip, ':');
    if (!port_str) {
        fprintf(stderr, "Invalid broadcast format\n");
        return NULL;
    }

    *port_str = '\0';  // Split ip and port
    int port = atoi(port_str + 1);

    return strdup(ip);
}

static float get_cpu_usage() {
    FILE* fp = fopen("/proc/stat", "r");
    if (!fp) return -1;

    unsigned long user, nice, system, idle;
    fscanf(fp, "cpu %lu %lu %lu %lu", &user, &nice, &system, &idle); // read cpu info
    fclose(fp);

    unsigned long total = user + nice + system + idle;
    static unsigned long prev_total = 0, prev_idle = 0;

    float usage = 0.0;
    if (prev_total > 0) {
        float diff_idle = idle - prev_idle;
        float diff_total = total - prev_total;
        usage = 100.0 * (1.0 - diff_idle / diff_total); // calculate cpu usage
    }

    prev_total = total;
    prev_idle = idle;
    return usage;
}

static float get_ram_usage() {
    FILE* fp = fopen("/proc/meminfo", "r");
    if (!fp) return -1;

    char line[128];
    unsigned long total = 0, free = 0;

    while (fgets(line, sizeof(line), fp)) {
        if (strstr(line, "MemTotal:")) sscanf(line, "MemTotal: %lu kB", &total);
        if (strstr(line, "MemFree:")) sscanf(line, "MemFree: %lu kB", &free);
    }
    fclose(fp);

    if (total == 0) return -1;
    return 100.0 * (total - free) / total;
}
</code></pre>
</details>


<a name="image_deep"></a>


## Подробнее об образах ОС

По мере погружения в разработку под одноплатники у меня начал нарастать интерес по устройству механизма развертывания операционной системы на плате.

Я уже демонстрировал в подразделе ["Установка образа операционной системы"](#install) самый простой вариант записи образа на носитель.

Там я использовал уже подготовленный для lichee_rv образ с кастомным ядром 6.1.0-d1-un-alt. Запуск с данным образом позволяет использовать всю необходимую перефирию: сеть и графику(хотя ее этому одноплатнику ой как тяжело тащить).

Но вообще, существуют и [регулярные сборки для riscv64](https://www.altlinux.org/Regular/riscv64) с более актуальным ядром. Другое дело, что интересующий нас SoC поддерживает регулярные сборки только теоретически. На деле, запись такого образа на lichee_rv позволит работать в системе, но без графики и сети, что ну..неудобно.

Кстати говоря, в статьях про [регулярные сборки](https://www.altlinux.org/Regular) и [регулярные сборки для riscv](https://www.altlinux.org/Ports/riscv64) упоминается инструмент [alt-rootfs-installer](https://www.altlinux.org/Write/rootfs). Использование данного инструмента позволяет легко запустить на плате Alt Linux с различными графическими оболочками. Для этого достаточно иметь архив с корневой файловой системой, а загрузчик и ядро, которое **запуститься** на нужном вам поддерживаемом устройстве утилита определит и поставит сама.

<a name="self_expirience_in_image"></a>

### Личный опыт создания образа для Lichee RV

Но все таки, мне хотелось бы понять и разобраться в технологии создания образа глубже. Собственно поделюсь тем, что узнал на данный момент.

Итак, что же нам нужно, чтобы получить образ с *нужным нам ядром, файловой системой и перефирией*:

 - архив с корневой файловой системой(rootfs)
 - исходники ядра(kernel source)
 - собранный u-boot
 - make и прочие сборочные зависимости
 - кросс-компилятор

 Теперь заново уконкретизирую задачу, чтобы было проще выстроить повествование. Я(мы!) хочу собрать образ **ALT Linux** для платы **Lichee RV** с более-менее свежим ядром, работающим wifi-модулем 

Отлично, возьмем [кросс-компилятор из репозитория](#cross-compiler), [регулярную сборку rootfs](https://www.altlinux.org/Regular/riscv64), заботливо собранный добрыми людьми, [готовый для работы u-boot](https://packages.altlinux.org/ru/sisyphus_riscv64/srpms/u-boot-sunxi-riscv/)(а иначе самому бы пришлось собирать!), [исходники ядра 6.16](https://packages.altlinux.org/ru/sisyphus/srpms/kernel-source-6.16/).


В случае необходимости установить ОС «Альт» на устройство без реализованных BIOS или EFI требуется дополнительный загрузчик. В основном для этой цели используется u-boot. Он берет на себя первоначальную инициализацию необходимых для загрузки устройств, загружает в память ядро linux, возможные initrd и dtb и передает управление ядру. Эта составляющая у нас уже есть. Останется только записать его на флеш-карту. Но об этом позже.

#### Сборка ядра

Далее, наверное, самая интригующая часть: сборка ядра. 

Сборка ядра Linux из исходных кодов — это процесс компиляции и компоновки исходного кода ядра в исполняемые файлы (ядро и загружаемые модули) на основе предоставленной вами конфигурации.

Предварительно, установим необходимые пакеты:

```
# apt-get install wget build-essential bc flex bison u-boot-tool ncurses-devel openssl libssl-devel dtc
```

Архив с исходным кодом получим в папке */usr/src/kernel/sources/kernel-source-6.16.tar*, установив соответствующий пакет.

Для этого переключимся на репозиторий [Sisyphus](https://www.altlinux.org/%D0%A7%D1%82%D0%BE_%D1%82%D0%B0%D0%BA%D0%BE%D0%B5_Sisyphus%3F) и выкачаем из него архивы с пакетами.
```
# apt-repo rm all

# apt-repo set sisyphus

# apt-get update
```
А затем установим нужный пакет

```
# apt-get install kernel-source-6.16
```

Распакуем в удобное место архив и перейдем в его корневую папку.

Далее, нам будет необходимо сконфигрировать файл .config, который собирает в себя все возможные параметры для сборки ядра.

Существует несколько способов создания такого файла. Воспользуемся классическим интерфейсом для make. Выполним
```
$ make menuconfig
```

Откроется интерактивный интерфейс для тонкой настройки, который позволяет представить древовидную структуру всех опций ядра (драйверы устройств, поддержка файловых систем, сетевые функции и т.д.) и выбирать нужные.

В menuconfig все навигируется стрелками. Выбор опций осуществляется с помощью Enter, включение (Y), выключение (N), установка опции как модуль (M) осуществляется с помощью соответствующих клавиш. Кроме того может пригодиться поиск нужных опций конфигурации и их зависимостей (/).

По завершении выбора нужных настроек все сохраняется в файл конфигурации .config.

#### Пример подготовки конфигурации

Приведу пример включения параметра PREEMPT_RT, отвечающего за включение в ядро модуля поддержки мягкого реального времени.

Для начала, можно в файл .config включить стандартную конфигурацию ядра, которая заполнит все параметры и теоретически позволяет запуститься ядру на любом устройстве указанной архитектуры. Сделать это можно так. 

```
$ make ARCH=riscv CROSS_COMPILE=riscv64-linux-gnu- defconfig
```
Эта команда готовит исходный код ядра Linux к сборке для архитектуры RISC-V, используя кросс-компиляцию. Рассмотрим подробнее, что мы передали в качестве аргументов и что будет выполнено в результате.

**ARCH=riscv** указывает целевую архитектуру процессора, для которой мы собираем ядро. Исходный код ядра Linux поддерживает десятки архитектур (x86_64, arm, arm64, powerpc, riscv и т.д.). Код, специфичный для каждой архитектуры, лежит в разных поддиректориях (например, arch/x86/, arch/arm/, arch/riscv/). Переменная ARCH говорит утилите make и системе сборки, к какой из этих директорий обращаться для поиска правильных исходников, конфигураций и правил сборки.

В данном случае мы говорим системе — "собирай не для моей родной архитектуры (скорее всего, x86_64), а для RISC-V".

**CROSS_COMPILE=riscv64-linux-gnu-** задает префикс для набора инструментов компилятора (toolchain), который используется для кросс-компиляции.

Для этого нужен специальный компилятор — кросс-компилятор. Его бинарные файлы обычно имеют префикс, указывающий на целевую архитектуру.

**riscv64-linux-gnu-** — это и есть тот самый префикс. Система сборки будет вызывать не просто gcc, а riscv64-linux-gnu-gcc; не ld, а riscv64-linux-gnu-ld и так далее. Об установке нужного кросс-компилятора прочитать можно [здесь](#cross-compiler).

**defconfig** — это цель (target) для make. Она генерирует базовый конфигурационный файл (.config) для выбранной архитектуры.

В директории arch/<arch>/configs/ (в нашем случае arch/riscv/configs/) лежат несколько заранее подготовленных конфигурационных файлов. defconfig — это обычно конфигурация по умолчанию для данной архитектуры. Она включает все необходимые опции для базовой работоспособности ядра на большинстве устройств с этой архитектурой, но без специфичных драйверов для какого-то конкретного железа.

Выполнение этой цели скопирует конфиг из arch/riscv/configs/defconfig в корневую директорию с исходным кодом ядра и сохранит его как файл .config, что и станет отправной точкой для нашей настройки.

Произведем донастройку получившейся конфигурации.

```
$ make ARCH=riscv menuconfig
```

P.S указание архитектуры здесь необходимо для того, чтобы система сборки обратилась к директории arch/riscv/ для получения всех архитектурно-зависимых настроек, списков плат и драйверов.

Итак, мы в графическом меню, и нам необходимо включить параметр *PREEMPT_RT* для того, чтобы добавить соответствующий модуль ядра в конфигурацию. Жмем (/) и пишем название параметра в появившееся поле ввода.

![alt text](/pictures/search_in_menuconfig.png)

Получив такой вывод можно сделать вывод о том,где в дереве расположен параметр и какие параметры с какими значениями нужно установить для возможности включения целевого параметра. В нашем случае необходимо чтобы параметр *EXPERT* был включен(он выключен), параметр *ARCH_SUPPORTS_RT* был включен(уже), а параметр *COMPILE_TEST* был отключен(тоже уже так).

Включив параметр *EXPERT* и найдя по указанному пути расположение параметра *PREEMPT_RT* мы получим желаемое.

Аналогично для того, чтобы обеспечить работу wifi-модуля Realtek 8723DS нужно будет найти и включить параметры RTW88_RTL8723DS. Кроме того, понадобиться указать в конфигурации, что сборка ведеся для платы с процессором Allwinner D1 от компании Sunxi, и включить поддержку различных аппаратных микросхем для нашей платы от компании Lichee RV.

*На момент публикации коммита (10.09.2025) у меня не вышло обеспечить работу wifi на плате. Но это, видимо связанно с неправильным device-tree, о чем расскажу подробнее позже.*

На данный момент я создал [репозиторий](https://github.com/Besogon1238/Researches_for_Lichee_RV), в котором складываю **все, что пригодилось мне или может пригодиться для сборки образа под Lichee RV**. Там будет все, кроме содержимого директории сборки ядра(она же первоначально директория с исходниками). В частности, сейчас там можно найти уже готовый конфиг для сборки ядра и образы самих ядер.

По завершении указанных действий, можно произвести попытку запуска сборки командой.

```
$ make ARCH=riscv CROSS_COMPILE=riscv64-linux-gnu- -j$(nproc)
```

Возможно, при этом make через CLI начнет задавать множество вопросов о конфигурации параметров, значение которых не было задано. Для этого предварительно нужно поменять конфиг, вызвав make с командой *olddefconfig*. В таком случае в наш конфиг для всех незаполненных параметров будут применены значения по умолчанию, как в defconfig.

Так что обобщенный порядок действий для нашего случая можно описать так:

```
$ cd ~/директория/сборки/

# загрузили в .config конфиг по умолчанию
$ make ARCH=riscv CROSS_COMPILE=riscv64-linux-gnu- defconfig

# сделали специфичные изменения
$ make ARCH=riscv menuconfig

# (опционально) заполнили активные, но не установленные параметры значениями по умолчанию

$ make ARCH=riscv CROSS_COMPILE=riscv64-linux-gnu- olddefconfig

# запустили сборку ядра

$ make ARCH=riscv CROSS_COMPILE=riscv64-linux-gnu- -j$(nproc)
```

После окончания сборки по пути *arch/riscv/boot/* можно будет найти бинарный файл готового ядра, а также его сжатый вариант с расширеним .gz. Кроме того в других подкаталогах будут сгенерированы внешние модули ядра.


#### Device Tree

После завершения сборки, в зависимости от того, поддержка каких SoC была выбраны в соответствующем разделе дерева параметров в директории сборки по пути *arch/архитектура/boot/dts/производитель* из файлов .dts будут скомпилированы файлы .dtb.

Статья на altlinux.org [прилагается](https://www.altlinux.org/Device_Tree).

Введу лишь несколько определений.

**Device Tree (DТ)** — это структура данных, которая описывает "железо" системы  для операционной системы. Позволяет одному ядру ОС работать на разных устройствах без перекомпиляции.

**DTS (Device Tree Source)** — исходный, человекочитаемый текстовый файл с описанием устройства (.dts или .dtsi для include-файлов).

**DTB (Device Tree Blob)** — бинарный, скомпилированный файл (.dtb), который загружается ядром ОС.

Основной источник(ресурс) dts — исходники ядра Linux и репозитории разработчиков железа. Файлы .dts лежат по пути arch/архитектура/boot/dts/производитель/ (например, arch/riscv/boot/dts/allwinner).

**Как такое написать самому?**
Если вдруг в наличии подходящего .dts файла нету, то нужно будет найти и изучить Datasheet для вашего SoC. Это главный источник информации об адресах периферии и их регистрах.

Как вариант, который приходит в голову - найти .dts файл для максимально похожего устройства в исходниках ядра и с ним уже что то придумывать. Я попробовал — и пока забросил это дело...ведь все что нужно для нашего случая уже есть.

Поскольку у меня в распоряжении плата Lichee RV **Dock**, и в исходниках ядра есть device tree от производителя, проблем с написанием исходного файла нет.

#### Загрузчик

...

#### rootfs

За примерами установки rootfs из архива на SD-карту или преобразования архива в готовый img-образ для конкретной платы с помощью alt-rootfs-installer предлагаю обратиться по ссылкам выше.         


#### Подготовка карты памяти к запуску образа


...


<a name="else_useful_info"></a>

## Потенциально полезные заметки 

Здесь размещается материал, который я не могу отнести к чему то конкретному, но опыт которого может быть применен мной или кем-то другим в будущем.

[Работа с логическим анализатором](https://github.com/Besogon1238/SSU-BaseALT/blob/main/Subpages/Additional_Subpages/Logic%20Analyser.md)

## Материалы к ознакомлению

### Linux на одноплатниках ARM64. Семинар в лаборатории.

Повседневно мы пользуемся какими-то прикладными программами. Но для работы им необходима операционная система, которая абстрагирует работу с физическим оборудованием. На этом семинаре расскажем как происходит загрузка ОС на одноплатниках с процессорами ARM и в чём этот процесс отличается от того, что происходит на amd64.

❓ На семинаре были рассмотрены следующие темы:

✅ Процесс загрузки систем arm64 и amd64.

✅ Погружение в стек технологий, используемых для загрузки компьютеров.

✅ Что нужно для того, чтобы Linux мог загрузиться на конкретном компьютере?

Дата проведения семинара: 24 апреля

Запись трансляции: [Linux на одноплатниках ARM64](https://vkvideo.ru/video-197903652_456239109)

Докладчик: Гущин Андрей — младший программист 2 категории